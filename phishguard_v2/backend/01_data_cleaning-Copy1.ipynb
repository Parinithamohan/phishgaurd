{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1668b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: (1030335, 2)\n",
      "After cleaning: (1030334, 2)\n",
      "CLEANED DATASET SAVED!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Load your combined dataset\n",
    "df = pd.read_csv(\"../master_dataset/MASTER_URL_DATASET.csv\")\n",
    "\n",
    "print(\"Before cleaning:\", df.shape)\n",
    "\n",
    "# ---------- 1. Remove duplicates ----------\n",
    "df.drop_duplicates(subset=[\"url\"], inplace=True)\n",
    "\n",
    "# ---------- 2. Remove empty / missing URLs ----------\n",
    "df = df[df[\"url\"].notna()]\n",
    "df = df[df[\"url\"].str.strip() != \"\"]\n",
    "\n",
    "# ---------- 3. Normalize HTTP/HTTPS ----------\n",
    "def normalize_url(u):\n",
    "    if not u.startswith(\"http\"):\n",
    "        return \"http://\" + u\n",
    "    return u\n",
    "\n",
    "df[\"url\"] = df[\"url\"].apply(normalize_url)\n",
    "\n",
    "# ---------- 4. Remove malformed URLs ----------\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df = df[df[\"url\"].apply(is_valid_url)]\n",
    "\n",
    "# ---------- 5. Remove extremely long URLs (rare, usually garbage) ----------\n",
    "df = df[df[\"url\"].str.len() < 250]\n",
    "\n",
    "# ---------- 6. Filter labels ----------\n",
    "df = df[df[\"label\"].isin([0, 1])]\n",
    "\n",
    "# ---------- 7. Reset index ----------\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(\"../master_dataset/MASTER_URL_DATASET_CLEANED.csv\", index=False)\n",
    "\n",
    "print(\"CLEANED DATASET SAVED!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b317177-756e-4a4d-ab66-f3595e4e60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (1030334, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to cache publicsuffix.org-tlds.{'urls': ('https://publicsuffix.org/list/public_suffix_list.dat', 'https://raw.githubusercontent.com/publicsuffix/list/master/public_suffix_list.dat'), 'fallback_to_snapshot': True} in C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tldextract\\.suffix_cache\\publicsuffix.org-tlds\\de84b5ca2167d4c83e38fb162f2e8738.tldextract.json. This could refresh the Public Suffix List over HTTP every app startup. Construct your `TLDExtract` with a writable `cache_dir` or set `cache_dir=None` to silence this warning. [WinError 5] Access is denied: 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\tldextract\\\\.suffix_cache'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE EXTRACTION COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "df = pd.read_csv(\"../master_dataset/MASTER_URL_DATASET_CLEANED.csv\")\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. BASIC LEXICAL FEATURES\n",
    "# -----------------------------------------\n",
    "\n",
    "df[\"url_length\"] = df[\"url\"].apply(len)\n",
    "df[\"num_digits\"] = df[\"url\"].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "df[\"num_letters\"] = df[\"url\"].apply(lambda x: sum(c.isalpha() for c in x))\n",
    "df[\"num_special\"] = df[\"url\"].apply(lambda x: sum(not c.isalnum() for c in x))\n",
    "\n",
    "special_chars = ['.', '-', '_', '@', '?', '=', '&', '%', '/', ':', '~', '#']\n",
    "\n",
    "for ch in special_chars:\n",
    "    df[f\"count_{ch.replace('.', 'dot')}\"] = df[\"url\"].apply(lambda x: x.count(ch))\n",
    "\n",
    "df[\"has_https\"] = df[\"url\"].apply(lambda x: 1 if x.startswith(\"https\") else 0)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. DOMAIN FEATURES\n",
    "# -----------------------------------------\n",
    "\n",
    "def extract_domain(url):\n",
    "    domain = tldextract.extract(url)\n",
    "    return domain.subdomain, domain.domain, domain.suffix\n",
    "\n",
    "df[\"subdomain\"], df[\"maindomain\"], df[\"tld\"] = zip(*df[\"url\"].apply(extract_domain))\n",
    "\n",
    "df[\"subdomain_length\"] = df[\"subdomain\"].apply(len)\n",
    "df[\"maindomain_length\"] = df[\"maindomain\"].apply(len)\n",
    "df[\"num_subdomain_dots\"] = df[\"subdomain\"].apply(lambda x: x.count('.'))\n",
    "\n",
    "# Suspicious TLDs\n",
    "suspicious_tlds = ['tk','ml','ga','cf','gq','xyz','zip','link','buzz','rest']\n",
    "\n",
    "df[\"tld_suspicious\"] = df[\"tld\"].apply(lambda x: 1 if x in suspicious_tlds else 0)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. ENTROPY (measures randomness)\n",
    "# -----------------------------------------\n",
    "\n",
    "from collections import Counter\n",
    "def entropy(string):\n",
    "    p, lns = Counter(string), float(len(string))\n",
    "    return -sum( count/lns * np.log2(count/lns) for count in p.values() )\n",
    "\n",
    "df[\"entropy\"] = df[\"url\"].apply(entropy)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. KEYWORD FEATURES (real phishing indicators)\n",
    "# -----------------------------------------\n",
    "\n",
    "keywords = [\n",
    "    \"login\",\"secure\",\"update\",\"verify\",\"account\",\"bank\",\"payment\",\n",
    "    \"signin\",\"checkout\",\"admin\",\"password\",\"billing\",\"support\",\n",
    "    \"helpdesk\",\"dropbox\",\"office\",\"microsoft\",\"apple\",\"google\"\n",
    "]\n",
    "\n",
    "for kw in keywords:\n",
    "    df[f\"kw_{kw}\"] = df[\"url\"].apply(lambda x: 1 if kw in x.lower() else 0)\n",
    "\n",
    "df[\"num_keywords\"] = df[[f\"kw_{kw}\" for kw in keywords]].sum(axis=1)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 5. URL COMPLEXITY FEATURES\n",
    "# -----------------------------------------\n",
    "\n",
    "df[\"num_slashes\"] = df[\"url\"].apply(lambda x: x.count('/'))\n",
    "df[\"num_params\"] = df[\"url\"].apply(lambda x: x.count('&'))\n",
    "df[\"num_fragments\"] = df[\"url\"].apply(lambda x: x.count('#'))\n",
    "df[\"num_question\"] = df[\"url\"].apply(lambda x: x.count('?'))\n",
    "df[\"num_equal\"] = df[\"url\"].apply(lambda x: x.count('='))\n",
    "\n",
    "# -----------------------------------------\n",
    "# SAVE\n",
    "# -----------------------------------------\n",
    "\n",
    "df.to_csv(\"../master_dataset/MASTER_WITH_FEATURES.csv\", index=False)\n",
    "print(\"FEATURE EXTRACTION COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9aea2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fab449d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\parin\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0142a209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n",
      "WARNING: Skipping keras as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip uninstall keras -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540bf64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parin\\AppData\\Local\\Temp\\ipykernel_31924\\3623000924.py:8: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../master_dataset/MASTER_WITH_FEATURES.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature dataset: (1030334, 52)\n",
      "Train: (824267, 47) Test: (206067, 47)\n",
      "\n",
      "Logistic Regression Accuracy: 0.9999514720940277\n",
      "Logistic Regression F1 Score: 0.9991751897063675\n",
      "\n",
      "XGBoost Accuracy: 0.9999611776752222\n",
      "XGBoost F1 Score: 0.9993402605970642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parin\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 2.0003e-04\n",
      "Epoch 2/2\n",
      "\u001b[1m2898/2898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.2448e-04 - val_accuracy: 1.0000 - val_loss: 1.5560e-04\n",
      "\u001b[1m6440/6440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step\n",
      "\n",
      "CNN Accuracy: 0.9999514720940277\n",
      "CNN F1 Score: 0.9991751897063675\n",
      "\n",
      "Models Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"../master_dataset/MASTER_WITH_FEATURES.csv\")\n",
    "print(\"Loaded feature dataset:\", df.shape)\n",
    "\n",
    "# -------------------------------------------\n",
    "# FEATURE SELECTION\n",
    "# -------------------------------------------\n",
    "\n",
    "ignore_cols = [\"url\", \"subdomain\", \"maindomain\", \"tld\"]\n",
    "features = [c for c in df.columns if c not in ignore_cols + [\"label\"]]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# -------------------------------------------\n",
    "# TRAIN-TEST SPLIT\n",
    "# -------------------------------------------\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# -------------------------------------------\n",
    "# MODEL A: LOGISTIC REGRESSION\n",
    "# -------------------------------------------\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=2000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "acc_lr = accuracy_score(y_test, pred_lr)\n",
    "f1_lr = f1_score(y_test, pred_lr)\n",
    "\n",
    "print(\"\\nLogistic Regression Accuracy:\", acc_lr)\n",
    "print(\"Logistic Regression F1 Score:\", f1_lr)\n",
    "\n",
    "# -------------------------------------------\n",
    "# MODEL B: XGBOOST\n",
    "# -------------------------------------------\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.7,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, pred_xgb)\n",
    "f1_xgb = f1_score(y_test, pred_xgb)\n",
    "\n",
    "print(\"\\nXGBoost Accuracy:\", acc_xgb)\n",
    "print(\"XGBoost F1 Score:\", f1_xgb)\n",
    "\n",
    "# -------------------------------------------\n",
    "# MODEL C: 1D CNN ON RAW URL CHARACTERS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Convert URL strings to padded sequences of integers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPool1D, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "url_texts = df[\"url\"].astype(str).tolist()\n",
    "\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(url_texts)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(url_texts)\n",
    "max_len = 200\n",
    "X_seq = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Train-test split for CNN\n",
    "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n",
    "    X_seq, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# CNN MODEL\n",
    "cnn = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "cnn.fit(X_seq_train, y_seq_train, epochs=2, batch_size=256, validation_split=0.1)\n",
    "\n",
    "pred_cnn = (cnn.predict(X_seq_test) > 0.5).astype(int)\n",
    "\n",
    "acc_cnn = accuracy_score(y_seq_test, pred_cnn)\n",
    "f1_cnn = f1_score(y_seq_test, pred_cnn)\n",
    "\n",
    "print(\"\\nCNN Accuracy:\", acc_cnn)\n",
    "print(\"CNN F1 Score:\", f1_cnn)\n",
    "\n",
    "# -------------------------------------------\n",
    "# SAVE MODELS\n",
    "# -------------------------------------------\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(log_reg, \"logistic_model.pkl\")\n",
    "joblib.dump(xgb, \"xgboost_model.pkl\")\n",
    "cnn.save(\"cnn_model.keras\")  # Save in native Keras format\n",
    "joblib.dump(tokenizer, \"cnn_tokenizer.pkl\")\n",
    "\n",
    "print(\"\\nModels Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2ff458-90fa-4bbb-a553-9e97ac7f4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns saved: 47\n"
     ]
    }
   ],
   "source": [
    "# Assume X_train is your training dataframe (47 features)\n",
    "import joblib\n",
    "\n",
    "feature_columns = X_train.columns.tolist()\n",
    "joblib.dump(feature_columns, \"models/feature_columns.pkl\")\n",
    "print(\"Feature columns saved:\", len(feature_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f344124-c845-4ce3-aab1-62d15e9327a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing probability: 0.5362428920406972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from feature_extractor import extract_features_for_single_url\n",
    "from model_loader import predict_soft_voting\n",
    "import numpy as np\n",
    "\n",
    "url = \"http://paypal-login-verification-secure-update.com\"\n",
    "fv = extract_features_for_single_url(url).values.astype(np.float32)\n",
    "fv = fv.reshape(1, -1)\n",
    "\n",
    "prob = predict_soft_voting(fv)\n",
    "print(\"Phishing probability:\", prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a298044a-f87a-477b-ae12-07d153e625cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector shape: (1, 47)\n"
     ]
    }
   ],
   "source": [
    "fv = extract_features_for_single_url(url).values\n",
    "print(\"Feature vector shape:\", fv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de92b5c3-7698-46f4-bde9-422e1a53c70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824267, 47)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a10abaaa-4ecf-41c6-ad73-86c919c109ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# numeric_features_vector: compute features exactly like you did in MASTER_WITH_FEATURES.csv\u001b[39;00m\n\u001b[0;32m      7\u001b[0m numeric_features_vector \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# list of 47 values\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m predict_soft_voting(url, numeric_features_vector)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\phishguard_v2\\backend\\model_loader.py:19\u001b[0m, in \u001b[0;36mpredict_soft_voting\u001b[1;34m(url, numeric_features_vector)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# ----- 1. Logistic & XGB -----\u001b[39;00m\n\u001b[0;32m     18\u001b[0m fv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(numeric_features_vector)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape (1, 47)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m p1 \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict_proba(fv)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m p2 \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict_proba(fv)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# ----- 2. CNN -----\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1431\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1423\u001b[0m ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     )\n\u001b[0;32m   1429\u001b[0m )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_predict_proba_lr(X)\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1433\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:397\u001b[0m, in \u001b[0;36mLinearClassifierMixin._predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n\u001b[0;32m    398\u001b[0m     expit(prob, out\u001b[38;5;241m=\u001b[39mprob)\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:363\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    360\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    361\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 363\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    364\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "from model_loader import predict_soft_voting\n",
    "\n",
    "# Example\n",
    "url = \"http://paypal-login-verification-secure-update.com\"\n",
    "\n",
    "# numeric_features_vector: compute features exactly like you did in MASTER_WITH_FEATURES.csv\n",
    "numeric_features_vector = [...]  # list of 47 values\n",
    "\n",
    "result = predict_soft_voting(url, numeric_features_vector)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8961312-dc39-4737-890a-75f90ba7d2eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(1, 1, 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m     11\u001b[0m     f \u001b[38;5;241m=\u001b[39m extract_features(url)  \u001b[38;5;66;03m# extract features for this URL\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     prob, score \u001b[38;5;241m=\u001b[39m predict_soft_voting(f)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprob\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\phishguard_v2\\backend\\model_loader.py:17\u001b[0m, in \u001b[0;36mpredict_soft_voting\u001b[1;34m(feature_vector)\u001b[0m\n\u001b[0;32m     12\u001b[0m def predict_soft_voting(url, numeric_features_vector):\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m     url: string, raw URL\n\u001b[0;32m     15\u001b[0m     numeric_features_vector: list or np.array of 47 features (for LogReg + XGB)\n\u001b[0;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m---> 17\u001b[0m     # ----- 1. Logistic & XGB -----\n\u001b[0;32m     18\u001b[0m     fv = np.array(numeric_features_vector).reshape(1, -1)  # shape (1, 47)\n\u001b[0;32m     19\u001b[0m     p1 = logreg.predict_proba(fv)[0][1]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m             data,\n\u001b[0;32m    869\u001b[0m             index,\n\u001b[0;32m    870\u001b[0m             columns,\n\u001b[0;32m    871\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    872\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    873\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:319\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    314\u001b[0m     values \u001b[38;5;241m=\u001b[39m _ensure_2d(values)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     values \u001b[38;5;241m=\u001b[39m sanitize_array(\n\u001b[0;32m    324\u001b[0m         values,\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m         allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:582\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     values \u001b[38;5;241m=\u001b[39m convert(values)\n\u001b[1;32m--> 582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_2d(values)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    590\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(1, 1, 57)"
     ]
    }
   ],
   "source": [
    "from model_loader import predict_soft_voting\n",
    "from feature_extractor import extract_features  # your existing function\n",
    "\n",
    "urls = [\n",
    "    \"https://google.com\",\n",
    "    \"http://paypal-login-verification-secure-update.com\",\n",
    "    \"https://openai.com\",\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    f = extract_features(url)  # extract features for this URL\n",
    "    prob, score = predict_soft_voting(f)\n",
    "    print(f\"{url} → {prob:.4f} → {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ddf07-17b4-43df-8733-4ddf08870737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154ebac-a7ea-4d02-a0be-443b017d5463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9911c9bc-d804-4216-9ddc-978ed0683b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f99379-d90b-4769-98c7-9ee252721c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2e16c-d5dc-4d54-a455-2f77d695889e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea301f-42d7-4b30-a862-bdb1636aad4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e07c56a3-5219-4b27-ba76-550fa521aab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parin\\AppData\\Local\\Temp\\ipykernel_31924\\3897635478.py:4: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../master_dataset/MASTER_WITH_FEATURES.csv\")  # adjust path if needed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030334, 47) (1030334,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "df = pd.read_csv(\"../master_dataset/MASTER_WITH_FEATURES.csv\")  # adjust path if needed\n",
    "feature_columns = joblib.load(\"models/feature_columns.pkl\")  # the same columns used for training\n",
    "\n",
    "# Keep only the required features\n",
    "X = df[feature_columns]\n",
    "y = df[\"label\"]  # make sure this column exists: 1=phishing, 0=safe\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff1f464e-42e1-4f96-910b-36fb3331ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48a934ea-e864-47fa-83ae-a486013db6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg test accuracy: 0.9999514720940277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Test accuracy\n",
    "print(\"LogReg test accuracy:\", logreg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "045fbd0f-b614-4b5e-ab21-179c2ed7e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parin\\AppData\\Roaming\\Python\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [23:58:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB test accuracy: 0.9999611776752222\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"XGB test accuracy:\", xgb_model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4963907-51b9-4454-9bca-a16d2eacbd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parin\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3ms/step - accuracy: 0.9992 - loss: 0.0043 - val_accuracy: 0.9999 - val_loss: 5.6431e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 4.1381e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.7994e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9999 - val_loss: 5.5675e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 9.2173e-04 - val_accuracy: 1.0000 - val_loss: 5.1586e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.7279e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.5638e-04 - val_accuracy: 1.0000 - val_loss: 6.5272e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 4.9785e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.8070e-04 - val_accuracy: 1.0000 - val_loss: 3.9728e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.9780e-04 - val_accuracy: 1.0000 - val_loss: 6.2969e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 8.9474e-04 - val_accuracy: 1.0000 - val_loss: 5.5992e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.4268e-04 - val_accuracy: 1.0000 - val_loss: 5.9447e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.5633e-04 - val_accuracy: 0.9999 - val_loss: 5.4116e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 8.2830e-04 - val_accuracy: 1.0000 - val_loss: 5.4314e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.6490e-04 - val_accuracy: 1.0000 - val_loss: 4.9745e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 9.7786e-04 - val_accuracy: 1.0000 - val_loss: 7.8577e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 5.6730e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 5.2299e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 6.1348e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m23183/23183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 5.3254e-04\n",
      "\u001b[1m6440/6440\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.6701e-04\n",
      "CNN test accuracy: [0.00046701388782821596, 0.9999514818191528]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "X_train_cnn = np.array(X_train)\n",
    "X_test_cnn = np.array(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_cnn.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train_cnn, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "print(\"CNN test accuracy:\", model.evaluate(X_test_cnn, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dce0bdc6-0e75-4462-aa4a-e11abd2fa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "logreg_path = \"models/logistic_model.pkl\"\n",
    "xgb_path = \"models/xgboost_model.pkl\"\n",
    "cnn_path = \"models/cnn_model.keras\"\n",
    "\n",
    "joblib.dump(logreg, logreg_path)\n",
    "joblib.dump(xgb_model, xgb_path)\n",
    "model.save(cnn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4b404-3830-447b-a089-c991fe7ec5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
